{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-18T10:18:26.699665Z",
     "start_time": "2025-01-18T10:18:26.697385Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, callbacks\n",
    "import numpy as np\n",
    "import music21\n",
    "import glob\n",
    "import keras\n",
    "\n",
    "from src.utils import (\n",
    "    parse_midi_files,\n",
    "    load_parsed_files,\n",
    "    get_midi_note,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:12:33.625941Z",
     "start_time": "2025-01-18T10:12:33.622983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PARSE_MIDI_FILES = False\n",
    "PARSED_DATA_PATH = \"data/parsed_sequences\"\n",
    "DATASET_REPETITIONS = 1\n",
    "\n",
    "SEQ_LEN = 50\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 5\n",
    "DROPOUT_RATE = 0.3\n",
    "FEED_FORWARD_DIM = 256\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# optimization\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "GENERATE_LEN = 50"
   ],
   "id": "5b9443f8dd931360",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:12:33.669716Z",
     "start_time": "2025-01-18T10:12:33.666919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_list = glob.glob(\"data/raw/*.mid\")\n",
    "print(f\"Found {len(file_list)} midi files\")"
   ],
   "id": "2168dd1487b5d63b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 midi files\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:12:34.824771Z",
     "start_time": "2025-01-18T10:12:33.712326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parser = music21.converter\n",
    "example_score = (\n",
    "    music21.converter.parse(file_list[1]).splitAtQuarterLength(12)[0].chordify()\n",
    ")"
   ],
   "id": "5baf49a5762f25d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:12:34.832380Z",
     "start_time": "2025-01-18T10:12:34.829800Z"
    }
   },
   "cell_type": "code",
   "source": "example_score.show(\"text\")",
   "id": "10bd4780cdef4e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0} <music21.metadata.Metadata object at 0x773fdd022f30>\n",
      "{0.0} <music21.stream.Measure 1 offset=0.0>\n",
      "    {0.0} <music21.instrument.Violoncello 'Solo Cello: Solo Cello'>\n",
      "    {0.0} <music21.instrument.Violoncello 'Violoncello'>\n",
      "    {0.0} <music21.clef.BassClef>\n",
      "    {0.0} <music21.tempo.MetronomeMark adagio Quarter=55>\n",
      "    {0.0} <music21.key.Key of E- major>\n",
      "    {0.0} <music21.meter.TimeSignature 4/4>\n",
      "    {0.0} <music21.chord.Chord C2 C3>\n",
      "    {1.0} <music21.chord.Chord C3>\n",
      "    {1.25} <music21.chord.Chord G2>\n",
      "    {1.5} <music21.chord.Chord A2>\n",
      "    {1.75} <music21.chord.Chord B2>\n",
      "    {2.0} <music21.chord.Chord C3>\n",
      "    {2.25} <music21.chord.Chord D3>\n",
      "    {2.5} <music21.chord.Chord E-3>\n",
      "    {2.75} <music21.chord.Chord F3>\n",
      "    {3.0} <music21.chord.Chord E-3>\n",
      "    {3.25} <music21.chord.Chord D3>\n",
      "    {3.5} <music21.chord.Chord E-3>\n",
      "    {3.75} <music21.chord.Chord C3>\n",
      "{4.0} <music21.stream.Measure 2 offset=4.0>\n",
      "    {0.0} <music21.chord.Chord C2 B2 F3 G#3>\n",
      "    {1.5} <music21.chord.Chord G#3>\n",
      "    {2.0} <music21.chord.Chord G3>\n",
      "    {2.5} <music21.chord.Chord G#3>\n",
      "    {2.75} <music21.chord.Chord F3>\n",
      "    {3.0} <music21.chord.Chord E-3>\n",
      "    {3.5} <music21.chord.Chord F3>\n",
      "    {3.75} <music21.chord.Chord D3>\n",
      "{8.0} <music21.stream.Measure 3 offset=8.0>\n",
      "    {0.0} <music21.chord.Chord C3 E-3>\n",
      "    {1.0} <music21.chord.Chord C2 E-3>\n",
      "    {2.0} <music21.chord.Chord C3>\n",
      "    {3.25} <music21.chord.Chord C3>\n",
      "    {3.5} <music21.chord.Chord D3>\n",
      "    {3.75} <music21.chord.Chord E-3>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:01.412211Z",
     "start_time": "2025-01-18T10:12:34.874198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if PARSE_MIDI_FILES:\n",
    "    notes, durations = parse_midi_files(\n",
    "        file_list, parser, SEQ_LEN + 1, PARSED_DATA_PATH\n",
    "    )\n",
    "else:\n",
    "    notes, durations = load_parsed_files(PARSED_DATA_PATH)\n",
    "len(notes),len(durations)"
   ],
   "id": "7e9b22b3fe33fe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Parsing data/raw/cs4-4sar.mid\n",
      "320 notes parsed\n",
      "2 Parsing data/raw/cs5-1pre.mid\n",
      "1629 notes parsed\n",
      "3 Parsing data/raw/cs2-4sar.mid\n",
      "1966 notes parsed\n",
      "4 Parsing data/raw/cs2-6gig.mid\n",
      "2701 notes parsed\n",
      "5 Parsing data/raw/cs4-5bou.mid\n",
      "4025 notes parsed\n",
      "6 Parsing data/raw/cs2-3cou.mid\n",
      "4766 notes parsed\n",
      "7 Parsing data/raw/cs5-5gav.mid\n",
      "5968 notes parsed\n",
      "8 Parsing data/raw/cs5-6gig.mid\n",
      "6417 notes parsed\n",
      "9 Parsing data/raw/cs3-2all.mid\n",
      "7154 notes parsed\n",
      "10 Parsing data/raw/cs4-3cou.mid\n",
      "8076 notes parsed\n",
      "11 Parsing data/raw/cs4-2all.mid\n",
      "9131 notes parsed\n",
      "12 Parsing data/raw/cs3-5bou.mid\n",
      "10012 notes parsed\n",
      "13 Parsing data/raw/cs5-2all.mid\n",
      "10704 notes parsed\n",
      "14 Parsing data/raw/cs6-5gav.mid\n",
      "11510 notes parsed\n",
      "15 Parsing data/raw/cs6-4sar.mid\n",
      "11845 notes parsed\n",
      "16 Parsing data/raw/cs3-6gig.mid\n",
      "12806 notes parsed\n",
      "17 Parsing data/raw/cs1-2all.mid\n",
      "13727 notes parsed\n",
      "18 Parsing data/raw/cs3-4sar.mid\n",
      "14068 notes parsed\n",
      "19 Parsing data/raw/cs3-1pre.mid\n",
      "15049 notes parsed\n",
      "20 Parsing data/raw/cs1-5men.mid\n",
      "15696 notes parsed\n",
      "21 Parsing data/raw/cs3-3cou.mid\n",
      "16685 notes parsed\n",
      "22 Parsing data/raw/cs1-6gig.mid\n",
      "17111 notes parsed\n",
      "23 Parsing data/raw/cs6-2all.mid\n",
      "17796 notes parsed\n",
      "24 Parsing data/raw/cs5-4sar.mid\n",
      "18015 notes parsed\n",
      "25 Parsing data/raw/cs6-1pre.mid\n",
      "19357 notes parsed\n",
      "26 Parsing data/raw/cs4-1pre.mid\n",
      "20174 notes parsed\n",
      "27 Parsing data/raw/cs2-2all.mid\n",
      "20867 notes parsed\n",
      "28 Parsing data/raw/cs2-1pre.mid\n",
      "21505 notes parsed\n",
      "29 Parsing data/raw/cs5-3cou.mid\n",
      "21935 notes parsed\n",
      "30 Parsing data/raw/cs2-5men.mid\n",
      "22462 notes parsed\n",
      "31 Parsing data/raw/cs6-3cou.mid\n",
      "23748 notes parsed\n",
      "32 Parsing data/raw/cs6-6gig.mid\n",
      "24920 notes parsed\n",
      "33 Parsing data/raw/cs1-1pre.mid\n",
      "25578 notes parsed\n",
      "34 Parsing data/raw/cs1-3cou.mid\n",
      "26398 notes parsed\n",
      "35 Parsing data/raw/cs1-4sar.mid\n",
      "26661 notes parsed\n",
      "36 Parsing data/raw/cs4-6gig.mid\n",
      "27632 notes parsed\n",
      "Building sequences of length 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27581, 27581)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:02.625849Z",
     "start_time": "2025-01-18T10:13:01.417921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(elements):\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(elements)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .shuffle(1000)\n",
    "    )\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=None, output_mode=\"int\"\n",
    "    )\n",
    "    vectorize_layer.adapt(ds)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    return ds, vectorize_layer, vocab\n",
    "\n",
    "\n",
    "notes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
    "durations_seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\n",
    "    durations\n",
    ");"
   ],
   "id": "f1a964656a1b2347",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737195181.600320   22573 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6139 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:2d:00.0, compute capability: 7.5\n",
      "2025-01-18 11:13:02.148112: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-01-18 11:13:02.621570: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:03.504322Z",
     "start_time": "2025-01-18T10:13:02.631695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(elements):\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(elements)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .shuffle(1000)\n",
    "    )\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=None, output_mode=\"int\"\n",
    "    )\n",
    "    vectorize_layer.adapt(ds)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    return ds, vectorize_layer, vocab\n",
    "\n",
    "\n",
    "notes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
    "durations_seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\n",
    "    durations\n",
    ")\n",
    "seq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))"
   ],
   "id": "2feede881223ea95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 11:13:03.498294: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:03.584217Z",
     "start_time": "2025-01-18T10:13:03.510452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_inputs(notes, durations):\n",
    "    notes = tf.expand_dims(notes, -1)\n",
    "    durations = tf.expand_dims(durations, -1)\n",
    "    tokenized_notes = notes_vectorize_layer(notes)\n",
    "    tokenized_durations = durations_vectorize_layer(durations)\n",
    "    x = (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
    "    y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
   ],
   "id": "5b5a0977198598a1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:03.601527Z",
     "start_time": "2025-01-18T10:13:03.590018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ],
   "id": "dc7cb33db57711d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:13:03.646957Z",
     "start_time": "2025-01-18T10:13:03.642200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        key_dim,\n",
    "        embed_dim,\n",
    "        ff_dim,\n",
    "        name,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "    ):\n",
    "        super(TransformerBlock, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "id": "bd85e2ebb5fa521d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:18:33.022140Z",
     "start_time": "2025-01-18T10:18:33.016964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_wavelength=10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # with other layers.\n",
    "        input_shape = tf.shape(inputs)\n",
    "        # length of sequence is the second last dimension of the inputs\n",
    "        seq_length = input_shape[-2]\n",
    "        hidden_size = input_shape[-1]\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "        # even indices are sine, odd are cosine\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        # embedding shape is [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        return tf.broadcast_to(positional_encodings, input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_wavelength\": self.max_wavelength,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "id": "b6e7265c6f8c3972",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:18:37.446199Z",
     "start_time": "2025-01-18T10:18:37.442396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            embeddings_initializer=\"he_uniform\",\n",
    "        )\n",
    "        self.pos_emb = SinePositionEncoding()\n",
    "\n",
    "    def call(self, x):\n",
    "        embedding = self.token_emb(x)\n",
    "        positions = self.pos_emb(embedding)\n",
    "        return embedding + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ],
   "id": "ffebe369cb2dad7d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:18:40.438070Z",
     "start_time": "2025-01-18T10:18:40.436087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "notes_vocab_size = len(notes_vocab)\n",
    "durations_vocab_size = len(durations_vocab)"
   ],
   "id": "7da8ef2772540b5a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:18:43.747445Z",
     "start_time": "2025-01-18T10:18:43.174598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "note_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "durations_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "note_embeddings = TokenAndPositionEmbedding(\n",
    "    notes_vocab_size, EMBEDDING_DIM // 2\n",
    ")(note_inputs)\n",
    "duration_embeddings = TokenAndPositionEmbedding(\n",
    "    durations_vocab_size, EMBEDDING_DIM // 2\n",
    ")(durations_inputs)\n",
    "embeddings = layers.Concatenate()([note_embeddings, duration_embeddings])\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM, name=\"attention\"\n",
    ")(embeddings)\n",
    "note_outputs = layers.Dense(\n",
    "    notes_vocab_size, activation=\"softmax\", name=\"note_outputs\"\n",
    ")(x)\n",
    "duration_outputs = layers.Dense(\n",
    "    durations_vocab_size, activation=\"softmax\", name=\"duration_outputs\"\n",
    ")(x)\n",
    "model = models.Model(\n",
    "    inputs=[note_inputs, durations_inputs],\n",
    "    outputs=[note_outputs, duration_outputs],  # attention_scores\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    \"adam\",\n",
    "    loss=[\n",
    "        losses.SparseCategoricalCrossentropy(),\n",
    "        losses.SparseCategoricalCrossentropy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "att_model = models.Model(\n",
    "    inputs=[note_inputs, durations_inputs], outputs=attention_scores\n",
    ")"
   ],
   "id": "a7c3bbf2c97e0b21",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1737195523.344302   25669 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/tcastillo/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/tcastillo/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/tcastillo/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "W0000 00:00:1737195523.351294   25674 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.352221   25668 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.353113   25670 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.354195   25669 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.355182   25678 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.356084   25672 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.357006   25673 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.357883   25671 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.358788   25675 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.359675   25676 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.360609   25679 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.362452   25677 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1737195523.369326   25674 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-01-18 11:18:43.689275: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION'\n",
      "\n",
      "/home/tcastillo/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'token_and_position_embedding_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''{{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: ''\n",
      "  warnings.warn(\n",
      "2025-01-18 11:18:43.689300: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-01-18 11:18:43.689310: W tensorflow/core/framework/op_kernel.cc:1829] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-01-18 11:18:43.714652: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION'\n",
      "\n",
      "2025-01-18 11:18:43.714673: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-01-18 11:18:43.714682: W tensorflow/core/framework/op_kernel.cc:1829] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling TokenAndPositionEmbedding.call().\n\n\u001B[1mInternalError.__init__() missing 2 required positional arguments: 'op' and 'message'\u001B[0m\n\nArguments received by TokenAndPositionEmbedding.call():\n  • args=('<KerasTensor shape=(None, None), dtype=int32, sparse=False, name=keras_tensor_2>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m note_inputs \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m,), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mint32)\n\u001B[1;32m      2\u001B[0m durations_inputs \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m,), dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mint32)\n\u001B[0;32m----> 3\u001B[0m note_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mTokenAndPositionEmbedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnotes_vocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEMBEDDING_DIM\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnote_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m duration_embeddings \u001B[38;5;241m=\u001B[39m TokenAndPositionEmbedding(\n\u001B[1;32m      7\u001B[0m     durations_vocab_size, EMBEDDING_DIM \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m      8\u001B[0m )(durations_inputs)\n\u001B[1;32m      9\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mConcatenate()([note_embeddings, duration_embeddings])\n",
      "File \u001B[0;32m~/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/MusicGenerationTransformer/.venv/lib/python3.12/site-packages/keras/src/ops/operation.py:80\u001B[0m, in \u001B[0;36mOperation.compute_output_spec\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m backend\u001B[38;5;241m.\u001B[39mcompute_output_spec(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 80\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m \u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCould not automatically infer the output shape / dtype of \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m (of type \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m). \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEither the `\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.call()` method \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     84\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mis incorrect, or you need to implement the \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m`\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.compute_output_spec() / \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompute_output_shape()` method. \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mError encountered:\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43me\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m new_e\u001B[38;5;241m.\u001B[39mwith_traceback(e\u001B[38;5;241m.\u001B[39m__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: Exception encountered when calling TokenAndPositionEmbedding.call().\n\n\u001B[1mInternalError.__init__() missing 2 required positional arguments: 'op' and 'message'\u001B[0m\n\nArguments received by TokenAndPositionEmbedding.call():\n  • args=('<KerasTensor shape=(None, None), dtype=int32, sparse=False, name=keras_tensor_2>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:19:04.053832Z",
     "start_time": "2025-01-18T10:19:04.051698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./model.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")"
   ],
   "id": "cfd682fdbd57a23e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T10:19:05.853106Z",
     "start_time": "2025-01-18T10:19:05.841112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    ds,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ],
   "id": "937041ee1945556f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mfit(\n\u001B[1;32m      2\u001B[0m     ds,\n\u001B[1;32m      3\u001B[0m     epochs\u001B[38;5;241m=\u001B[39mEPOCHS,\n\u001B[1;32m      4\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
